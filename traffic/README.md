While training this traffic sign recognition model, I experimented with a lot of configurations to find the best balance of performance and efficiency. I tested the models with multiple convolutional and pooling layers, increased the number of filters in each layer (e.g., 16, 32, 64), and kept adjusting the kernel sizes as well. Although the deeper models showed some improvement in training accuracy, they regularly led to overfitting, with a lower performance on the testing sets.

I also experimented with different dense layer sizes (64, 128, 256 units) and dropout rates (0.3, 0.5, 0.7) to improve generalization. From what I saw, higher dropout rates reduced overfitting but sometimes hurt performance. Ultimately, I settled on a simpler architecture: one convolutional layer with 32 filters, a pooling layer, a dense layer with 128 units, and a dropout rate of 0.5. With this configuration, which closely resembled the example settings from the lecture, I achieved the best testing accuracy.
